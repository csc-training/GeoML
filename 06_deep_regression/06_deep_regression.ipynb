{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**TODO:**\n",
    "* make sure to make it possible to go through this in 1:00\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Regression\n",
    "\n",
    "Reminder: We are within supervised learning (we have labels/targets that are real values) -> Regression\n",
    "\n",
    "Data and goal: In this notebook we read the zip code data produced by **02_vector_preparations** and create one deep learning model for\n",
    "predicting the average zip code income from population and spatial features. We will try to tune hyperparameters and finally assesses the models performance metrics on a previously unseen test dataset.\n",
    "\n",
    "Contents of this notebook:\n",
    "\n",
    "0. Prepare environment\n",
    "1. Reading the data\n",
    "2. Check for GPU\n",
    "3. Model definition\n",
    "4. Task\n",
    "5. Performance\n",
    "6. Comparison to shallow models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "#plotting loss\n",
    "import matplotlib.pyplot as plt\n",
    "# error metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "# deep learning tools\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the data\n",
    "### 1.1 Define input and output file paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.environ.get('USER')\n",
    "base_directory= f'/scratch/project_2002044/{username}/2022/GeoML'\n",
    "\n",
    "data_directory = os.path.join(base_directory,'data')\n",
    "\n",
    "#inputs\n",
    "preprocessed_data_directory = os.path.join(data_directory,'preprocessed_regression')\n",
    "train_dataset_name = os.path.join(preprocessed_data_directory,'scaled_train_zip_code_data.csv')\n",
    "test_dataset_name = os.path.join(preprocessed_data_directory,'scaled_test_zip_code_data.csv')\n",
    "val_dataset_name = os.path.join(preprocessed_data_directory,'scaled_val_zip_code_data.csv')\n",
    "train_label_name = os.path.join(preprocessed_data_directory,'train_income_labels.pkl')\n",
    "test_label_name = os.path.join(preprocessed_data_directory,'test_income_labels.pkl')\n",
    "val_label_name = os.path.join(preprocessed_data_directory,'val_income_labels.pkl')\n",
    "\n",
    "# outputs\n",
    "results_directory = os.path.join(data_directory,'regression_results')\n",
    "metrics_filename = os.path.join(results_directory,'shallow_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducible results when randomness is involved, we can set a random seed\n",
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train, validation and test datasets\n",
    "x_train = pd.read_csv(train_dataset_name)\n",
    "x_val = pd.read_csv(val_dataset_name)\n",
    "y_train = pd.read_pickle(train_label_name)\n",
    "y_val = pd.read_pickle(val_label_name)\n",
    "x_test = pd.read_csv(test_dataset_name)\n",
    "y_test = pd.read_pickle(test_label_name)\n",
    "num_of_x_columns =  x_train.to_numpy().shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for GPUs\n",
    "\n",
    "In this part of the course we do not yet need GPUs, the dataset and models used are sufficiently small, and training goes fast also on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkGPUavailability():\n",
    "    device = tensorflow.config.list_physical_devices('GPU')\n",
    "    if device:\n",
    "        print(\"We have a GPU available!\")\n",
    "    else:\n",
    "        print(\"Sadly no GPU available. :( you have settle with a CPU. Good luck!\")\n",
    "\n",
    "checkGPUavailability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model definition\n",
    "\n",
    "We will now build a model (linear stack of layers) from scratch using [keras Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "We start with a model with one input and one output layer and two hidden layers. \n",
    "The input shape for the input layer is defined by the number of features available.\n",
    "The number of layers and perceptrons/neurons/nodes per layer can be chosen freely. We will adjust them later.\n",
    "We choose [ReLU - rectified linear unit](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation function.\n",
    "The last layer is also called output layer. Since we are doing regression, we only want one value per set of features for the average income per zip-code.\n",
    "To [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) the model, we set the optimizer to the default [RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) with default learning rate of 0.001. We use the mean square error to compute the loss. We set the mean average error and mean square error to be evaluated by the model during training and testing.  \n",
    "\n",
    "Then we [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) the model to the training data. The number of epochs is set to 100; one epoch is one iteration over the entire x and y data provided. We want to shuffle the training data before each epoch `shuffle=True. And set the batch size to 64, which sets the number of samples that are forwarded through the model in each pass. We also set what dataset to use as validation dataset. We store the whole model fitting into the variable history to visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add first layer with 64 perceptrons. Activation function is relu\n",
    "model.add(Dense(64, activation='relu', input_shape=(num_of_x_columns,)))\n",
    "\n",
    "# Add another layer with 64 perceptrons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# The last layer has to have only 1 perceptron as it is the output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Setting optimizer and loss functions. Learning rate set to 0.001\n",
    "model.compile(optimizer=RMSprop(learning_rate=.001), loss='mse', metrics=['mae','mse'])\n",
    "print(model.summary())\n",
    "\n",
    "#Train the network with 100 epochs and batch size of 64, store it into history, for loss plot\n",
    "history = model.fit(x_train, y_train, epochs=100, shuffle=True, batch_size=64, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot the loss for visualization of above training progress\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  #plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can you improve on the performance metrics?\n",
    "You can add layers to the model or change the number of epochs, number of neurons per layer or batch size. What can you observe?\n",
    "Also compare to your results from the shallow regression exercise.\n",
    "Report the best results (on test set below) and mark down parameters used, so that others can reproduce the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance\n",
    "\n",
    "After tuning the model based on the validation data, we can use the test set to report the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the performance of the model using previously unseen test dataset\n",
    "prediction = model.predict(x_test)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "rmse = sqrt(mean_squared_error(y_test, prediction))\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "\n",
    "print(\"\\nMODEL ACCURACY METRICS WITH TEST DATASET: \\n\" +\n",
    "        \"\\t Root mean squared error: \"+ str(rmse) + \"\\n\" +\n",
    "        \"\\t Mean absolute error: \" + str(mae) + \"\\n\" +\n",
    "        \"\\t Coefficient of determination: \" + str(r2) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison to shallow models and baseline \n",
    "\n",
    "Now we can print again the performance metrics for our models from the shallow regression exercises and see how the deep model is performing in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shallow_metrics = pd.read_csv(metrics_filename)\n",
    "\n",
    "print(shallow_metrics.sort_values(by=['RMSE'], ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
