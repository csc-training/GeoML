{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector data for exercises\n",
    "\n",
    "In this course, we will use the vector dataset **Paavo**, which represents postal code area statistics collected by [Statistics Finland](https://www.stat.fi). Metadata description can be found on [Statistics Finland webpage](https://www.stat.fi/static/media/uploads/tup/paavo/paavo_kuvaus_en.pdf), see page 5 ff for field name descriptions.\n",
    "\n",
    "The dataset includes variables about each postcode area, describing:\n",
    "\n",
    "1. Population Structure (24 variables) HE\n",
    "2. Educational Structure (7 variables) KO\n",
    "3. Inhabitants' Disposable Monetary Income (7 variables) HR\n",
    "4. Size and Stage in Life of Households (15 variables) TE\n",
    "5. Households' Disposable Monetary Income (7 variables) TR\n",
    "6. Buildings and Dwellings (8 variables) RA\n",
    "7. Workplace Structure (26 variables) TP\n",
    "8. Main Type of Activity (9 variables) PT\n",
    "\n",
    "The overall goal of the exercises is to predict the median income for each zip code based on other variables/features of the dataset. \n",
    "This exercise is meant to show the different steps to prepare a vector dataset for machine learning. To make this task worth an exercise, all variables/features of type HR (that tell about the income) are removed from the dataset.\n",
    "\n",
    "## Vector data preparations\n",
    "\n",
    "Content of this notebook:\n",
    "\n",
    "0. Environment preparation\n",
    "1. Data retrieval\n",
    "2. Data exploration\n",
    "3. Data cleaning\n",
    "4. Feature engineering\n",
    "5. Feature encoding\n",
    "6. Train/Test split\n",
    "7. Feature scaling\n",
    "8. Store the results\n",
    "\n",
    "In this notebook we will prepare the Paavo dataset for Machine Learning, by downloading all the necessary datasets, clean up some features, join auxiliary\n",
    "data and encode text fields, split the dataset into train and test set and scale the features for machine learning purposes. \n",
    "\n",
    "The goal of this exercise is to get the dataset ready for subsequent machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment preparation\n",
    "\n",
    "Load all the needed Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# operating system level operations\n",
    "import os\n",
    "# for file operations\n",
    "import shutil\n",
    "# filesystem exploration\n",
    "import glob\n",
    "# unpacking compressed files\n",
    "import zipfile\n",
    "# timing operations\n",
    "import time\n",
    "# data handling (and plotting)\n",
    "import pandas as pd\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# geospatial data handling \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon\n",
    "# Machine learning data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder \n",
    "# download data from URL\n",
    "from urllib.request import urlretrieve\n",
    "# for saving the scaler, uncomment following:\n",
    "# from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducible results when randomness is involved, we can set a random seed\n",
    "random_seed= 63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Data retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Creating directories\n",
    "Let's create a data directory in the base of our GeoML directory, where we store the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter for courses cloned (\"copied\") the course material for us into '/scratch/project_2002044/training_xxx/2022/GeoML', we need to define that path\n",
    "username = os.environ.get('USER')\n",
    "base_directory= f'/scratch/project_2002044/{username}/2022/GeoML'\n",
    "\n",
    "def create_dir(directory_name):\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "data_directory = os.path.join(base_directory,'data')\n",
    "paavo_directory=os.path.join(data_directory,'paavo')\n",
    "maakunta_directory=os.path.join(data_directory,'maakunta')\n",
    "preprocessed_data_directory = os.path.join(data_directory,'preprocessed_regression')\n",
    "\n",
    "# make sure, all needed directories are created in beginning \n",
    "create_dir(data_directory)\n",
    "create_dir(paavo_directory)\n",
    "create_dir(maakunta_directory)\n",
    "create_dir(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Getting data\n",
    "Let's get the original datasets that we need for this exercise from Puhtis `data` (read only) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puhti_data_directory = '/appl/data/geo'\n",
    "\n",
    "def copy_files(source,destination):\n",
    "    for file in glob.glob(source):\n",
    "        print(file)\n",
    "        shutil.copy(file, destination)\n",
    "               \n",
    "copy_files(os.path.join(puhti_data_directory, 'tilastokeskus/paavo/2022/pno_tilasto_2022.*'), paavo_directory)\n",
    "copy_files(os.path.join(puhti_data_directory, 'mml/hallintorajat_10k/2021_2022/SuomenMaakuntajako_2021_10k.*'), maakunta_directory)\n",
    "\n",
    "#when working on your own, it is often preferred to not copy, but read the data directly from source\n",
    "#from Puhti\n",
    "#original_gdf = gpd.read_file('/appl/data/geo/tilastokeskus/paavo/2022/pno_tilasto_2022.shp', encoding='utf-8')    \n",
    "#from Paituli\n",
    "#original_gdf = gpd.read_file('/vsicurl/https://www.nic.funet.fi/index/geodata/tilastokeskus/paavo/2022/pno_tilasto_2022.shp', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Storing file locations\n",
    "\n",
    "In order to use the data later, we store their path and filename in variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputfiles\n",
    "paavo_shapefile = os.path.join(paavo_directory,'pno_tilasto_2022.shp')\n",
    "finnish_regions_shapefile = os.path.join(maakunta_directory, 'SuomenMaakuntajako_2021_10k.shp')\n",
    "\n",
    "#outputfiles\n",
    "scaled_train_dataset_name = os.path.join(preprocessed_data_directory,'scaled_train_zip_code_data.csv')\n",
    "scaled_test_dataset_name = os.path.join(preprocessed_data_directory,'scaled_test_zip_code_data.csv')\n",
    "scaled_val_dataset_name = os.path.join(preprocessed_data_directory,'scaled_val_zip_code_data.csv')\n",
    "train_label_name = os.path.join(preprocessed_data_directory,'train_income_labels.pkl')\n",
    "test_label_name = os.path.join(preprocessed_data_directory,'test_income_labels.pkl')\n",
    "val_label_name = os.path.join(preprocessed_data_directory,'val_income_labels.pkl')\n",
    "train_dataset_name = os.path.join(preprocessed_data_directory,'train_zip_code_data.csv')\n",
    "test_dataset_name = os.path.join(preprocessed_data_directory,'test_zip_code_data.csv')\n",
    "val_dataset_name = os.path.join(preprocessed_data_directory,'val_zip_code_data.csv')\n",
    "\n",
    "# optional to store the scaler\n",
    "#scaler_path = '../original_data/paavo/zip_code_scaler.bin'\n",
    "\n",
    "# optional to store train, validation and test as geopackages for visualization\n",
    "train_dataset_geo = os.path.join(preprocessed_data_directory,'train_zip_code_data.gpkg')\n",
    "val_dataset_geo = os.path.join(preprocessed_data_directory,'val_zip_code_data.gpkg')\n",
    "test_dataset_geo = os.path.join(preprocessed_data_directory,'test_zip_code_data.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration\n",
    "\n",
    "Always get to know your data before even thinking about Machine Learning. This section shows a few ways that we can get to know Paavo dataset a bit better. Possibilities are endless. For some models, you should also check that assumptions the model makes about data distribution are true.\n",
    "\n",
    "### 2.1 Read the data into dataframe\n",
    "\n",
    "Read the zip code dataset into a geopandas dataframe `original_gdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the encoding makes sure that characters are represented as intended, important especially with languages that have \"special characters\" \n",
    "original_gdf = gpd.read_file(paavo_shapefile, encoding='utf-8')\n",
    "original_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploring the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe columns and rows\n",
    "print(f\"Original dataframe size: {len(original_gdf.index)} rows (= zip codes) with {len(original_gdf.columns)} columns (=variables/features)\")\n",
    "# column names\n",
    "print(list(original_gdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column data types\n",
    "print(list(pd.unique(original_gdf.dtypes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nodata isna/null gives True/False , summing gives +1 for each True for each column, summing again gives total amount of True for the whole dataframe\n",
    "print(original_gdf.isna().sum().sum())\n",
    "print(original_gdf.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unsensible cells\n",
    "# e.g. income equal to or below 0\n",
    "print(len(original_gdf[original_gdf[\"hr_mtu\"]==0]))\n",
    "print(len(original_gdf[original_gdf[\"hr_mtu\"]<0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get value range of each column\n",
    "for x in original_gdf.columns:\n",
    "    if original_gdf[x].dtype in ['int64','float64']:\n",
    "        min = original_gdf[x].min()\n",
    "        max = original_gdf[x].max()\n",
    "        print(f'Value range of {x} : {str(min)} to {str(max)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like `-1` is used as no data value. We can keep and remember this as is, or replace all `-1` with `np.nan` which can for example later be interpolated or removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization\n",
    "\n",
    "Another way of data exploration is visualizing different features of your dataset in different ways to reveal phenonemons that might not be visible when looking at numbers only.\n",
    "\n",
    "#### 2.3.1 Distribution plot\n",
    "\n",
    "In this exercise, we are interested in the income per capita. So let's check out the distribution of that target feature by plotting a distribution plot with seaborn `distplot` functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder on how to get help within notebook\n",
    "help(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using seaborn histogram plot\n",
    "sns.histplot(original_gdf['hr_mtu'])\n",
    "# other option:\n",
    "#original_gdf['hr_mtu'].hist()\n",
    "# another option to identify outliers would be to use boxplot \n",
    "#sns.boxplot(original_gdf['hr_mtu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some zip codes have an income of 0, which in this case probably means \"no data available\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Map\n",
    "As we are working with spatial data, we can also plot a map of the target feature to explore its spatial distribution.\n",
    "\n",
    "If plotting maps with matplotlib is not familiar. Here are some things you can play with\n",
    "* **figsize** - different height, width\n",
    "* **column** - try other features\n",
    "* **cmap** - this is the color map, here are the possibile options https://matplotlib.org/3.3.1/tutorials/colors/colormaps.html\n",
    "\n",
    "The following plots are only for quick visualization, to include these plots in publications, more features would need to be taken care of ( such as axes and their labels, north arrow, colorblind and print friendly color palette,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check coordinate reference frame\n",
    "print(original_gdf.crs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# set title for the full plot\n",
    "ax.set_title(\"Average income by zip code\", fontsize=25)\n",
    "# turn off all axes\n",
    "ax.set_axis_off()\n",
    "# plot the average income\n",
    "plot = original_gdf.plot(column='hr_mtu', ax=ax, legend=True, cmap=\"magma\")\n",
    "# set colorbar label\n",
    "cax = fig.get_axes()[1]\n",
    "cax.set_ylabel('Income in â‚¬');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Regression plots\n",
    "\n",
    "We can also explore, how the different features are related to another by plotting them \"against each other\" by plotting some regression plots, i.e. scatter plots with a \"best fitting\" regression line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some variables\n",
    "variables = ['euref_x', 'euref_y', 'he_kika','he_miehet']\n",
    "fig,ax = plt.subplots(2,2)\n",
    "# to fit all titles and axes labels\n",
    "fig.tight_layout(h_pad=5,w_pad=5)\n",
    "\n",
    "# ravel axes to loop through and fill subplots one by one\n",
    "for var,axes in zip(variables, ax.ravel()):\n",
    "    # Regression Plot also by default includes best-fitting regression line which can be turned off via `fit_reg=False`\n",
    "    sns.regplot(x=var, y='hr_mtu', data=original_gdf,  marker='.', scatter_kws = {'s': 10},ax = axes).set(title=f'Regression plot of \\n {var} and average income');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning \n",
    "\n",
    "We cann check for empty rows and columns as well as empty single cells and either remove them from the dataset or, if domain knowledge allows, fill them with sensible values. Note that this might have significant impact on the results. So fill with care, and if unsure, rather remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that have missing values or where average income is -1 (=not known) or 0\n",
    "selected_gdf = original_gdf.dropna()\n",
    "selected_gdf = selected_gdf[selected_gdf[\"hr_mtu\"]>0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataframe size after dropping no data (for income) rows: {len(selected_gdf.index)} zip codes with {len(selected_gdf.columns)} columns\")\n",
    "\n",
    "# Remove some columns that are strings (nimi, namn, kunta = name of the municipality in Finnish and Swedish)\n",
    "# or which might make the modeling too easy as directly realted to inhabitants income ('hr_mtu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy') or household income ('tr_ktu', 'tr_mtu')\n",
    "columns_to_be_removed_completely = ['nimi','namn','kunta','hr_ktu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy', 'tr_ktu', 'tr_mtu']\n",
    "selected_gdf = selected_gdf.drop(columns_to_be_removed_completely,axis=1)\n",
    "\n",
    "print(f\"Dataframe size after dropping columns with string values and columns that make modeling too easy : {len(selected_gdf.index)} zip codes with {len(selected_gdf.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "This section does not include any computations, as this goes out of scope of the course and is mainly geospatial processing that is not specific to machine learning.\n",
    "\n",
    "Sometimes, the features as they come from the dataset can be further refined to represent the data in a way that is easier to use for modelling. One example would be to calculate the ratio or some other statistical measure of two or multiple features. This step requires domain knowledge to find sensible features. In this step you can also again think about, what additional datasets could be used to add information for the task.\n",
    "\n",
    "In the spatial domain, incorporating the neighborhood of each zip-code into features or describing the shape of polygons with values could be ways of feature engineering.\n",
    "\n",
    "For example, we expect (=domain knowledge) that people with higher income can afford to live in cities and near lakes or some national park. So we could also engineer some features that represent these factors with additional datasets:\n",
    "\n",
    "* distance to closest city center, which could be derived from [naturalearth populated places dataset](https://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-populated-places/)\n",
    "* number of lakes in the area (e.g. 5km radius), see e.g. [SYKE water areas](https://ckan.ymparisto.fi/dataset/%7BAD287567-30F9-4529-9B47-2D6573FCAA9E%7D)\n",
    "* distance to closest national park, see e.g. [SYKE state areas of natural protection](https://ckan.ymparisto.fi/dataset/%7BC8FC4A42-A2C3-40C4-92CD-2299C688514E%7D)\n",
    "\n",
    "In the temporal domain, if we are working with timeseries, but not with specific time series models, we could create features representing the temporal domain, such as the ratio of a two timepoints of the same variable.\n",
    "\n",
    "Be creative! \n",
    "\n",
    "> Note: Make sure that you can create the same features also for future datasets that you might want to apply your model to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature encoding\n",
    "\n",
    "* Most Machine Learning algorithms cannot handle categorical features per se, they have to be converted to numerical values\n",
    "* Categorical features can be binary (True/False, 1/0), ordinal (low,medium,high) or nominal (monkey, donkey, tiger, penguin)\n",
    "\n",
    "To practice, we can add region names to the post codes. One of the most-used encoding techniques is **one-hot encoding**. This means that instead of one column with different names, we create <number of unique values in column> new columns and fill then with 1/0. \n",
    "-> Same information content but numerical cells and no hierarchy (as we would get when simply assigning a numerical value to each string) \n",
    "-> also called \"dummy variables\"\n",
    "\n",
    "We use the pandas **get_dummies()** function for one-hot encoding. Scikit would also have a **OneHotEncoder()** transformer for this\n",
    "\n",
    "* More information on one-hot encoding https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "* It might not always be the best option. See other options https://towardsdatascience.com/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Spatially join the region information to the dataset \n",
    "\n",
    "First we need to bring the two dataframes together. We want to know which region each zip code are is in, so we want to \"spatially join\" the two dataframes. As the zip code areas might overlap several regions, let's choose that region for each zip code, where the mid point of each zip code polygon falls in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the regions shapefile and select only the name of the region and its geometry\n",
    "finnish_regions_gdf = gpd.read_file(finnish_regions_shapefile)\n",
    "finnish_regions_gdf = finnish_regions_gdf[['NAMEFIN','geometry']]\n",
    "\n",
    "# A function we use to return centroid point geometry from a zip code polygon\n",
    "def returnPointGeometryFromXY(polygon_geometry):\n",
    "    ## Calculate x and y of the centroid\n",
    "    centroid_x,centroid_y = polygon_geometry.centroid.x,polygon_geometry.centroid.y\n",
    "    ## Create a shapely Point geometry of the x and y coords\n",
    "    point_geometry = Point(centroid_x,centroid_y)\n",
    "    return point_geometry\n",
    "\n",
    "# Stash the polygon geometry to another column as we are going to overwrite the 'geometry' with centroid geometry\n",
    "selected_gdf['polygon_geometry'] = selected_gdf['geometry']\n",
    "\n",
    "# We will be joining the region name to zip codes according to the zip code centroid. \n",
    "# This calls the function above and returns centroid to every row\n",
    "selected_gdf[\"geometry\"] = selected_gdf['geometry'].apply(returnPointGeometryFromXY)\n",
    "\n",
    "# Spatially join the region name to the zip codes using the centroid of zip codes and region polygons\n",
    "selected_and_joined_gdf = gpd.sjoin(selected_gdf,finnish_regions_gdf,how='inner',predicate='intersects')\n",
    "# look at the end of the dataframe to see if it worked (the beginning of the dataframe has too many zip codes in same area)\n",
    "selected_and_joined_gdf.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here onwards, we do not need the geometry of the zip code areas anymore and can remove them from the dataframe.\n",
    "If you want to visualize the resulting datasets with geometries later, you can join them back via the zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_and_joined_gdf.drop(['index_right','polygon_geometry', 'geometry'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 One-hot encode the region name\n",
    "\n",
    "Let's practice now the one-hot encoding on the spatially joined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the region name with One-hot encoder (= in pandas, dummy encoding)\n",
    "ohencoder = OneHotEncoder()\n",
    "encoded_gdf = pd.get_dummies(selected_and_joined_gdf['NAMEFIN'])\n",
    "\n",
    "col_names_no_scaling = list(encoded_gdf.columns)\n",
    "\n",
    "# Join original gdf and encoded gdf together, drop the original finnish name column\n",
    "new_encoded_gdf = selected_and_joined_gdf.join(encoded_gdf).drop('NAMEFIN',axis=1)\n",
    "\n",
    "print(\"Dataframe size after adding region name: \" + str(len(new_encoded_gdf.index))+ \" zip codes with \" + str(len(new_encoded_gdf.columns)) + \" columns\")\n",
    "\n",
    "# Print the tail of the dataframe\n",
    "new_encoded_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train- Test split\n",
    "\n",
    "In order to determine later, how well our models perform on previously unseen data we need to split the dataset into so-called `train`, `test` and `validation` dataset. We use the `train` dataset during model training, so our regressor gets to know that dataset really well. Then we will use our `validation` dataset to finetune the parameters of our models, i.e. we use knowledge gained from applying the trained model on unseen data to adapt the parameters. That means that this dataset is no longer unknown to the model. So we need a third new dataset (`test`) to finally test how well the model performs on previously unseen data.\n",
    "\n",
    "![](../images/supervised_workflow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the gdf to x (the predictor attributes) and y (the attribute to be predicted)\n",
    "y = new_encoded_gdf['hr_mtu'] # Average income\n",
    "\n",
    "# Remove label\n",
    "x = new_encoded_gdf.drop(['hr_mtu'],axis=1)\n",
    "\n",
    "# Split both datasets to train (60%) and test (40%) datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.4, random_state=random_seed)\n",
    "\n",
    "# Split the test dataset in half, to get 20% validation and 20% test dataset\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=.5, random_state=random_seed)\n",
    "\n",
    "# \\n tells the print command to start a new line\n",
    "print(f'Shape of train dataset: {x_train.shape} \\n Shape of test dataset: {x_test.shape} \\n Shape of validation dataset: {x_val.shape}')\n",
    "        \n",
    "x_train = x_train.reset_index(drop=True)\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "x_val = x_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "Feature Scaling is one of the most important data preparation steps. This is to avoid biasing algorithms that compute distances between features (e.g. like KNN, SVM and other non-treebased) towards numerically larger values. Feature scaling also helps the algorithm to train and converge faster.\n",
    "The most popoular scaling techniques are normalization and standardization. Both scale the values of the current cell based on all given other cells, this means that scaling has to be done before train/test split to avoid bias towards unseen data. Apply to test set afterwards.\n",
    "\n",
    "## 7.1 Normalization or min-max scaling \n",
    "\n",
    "* X_new = (X - X_min)/(X_max - X_min)\n",
    "* Used when features are of different scales, eg average size of household (te_takk) and number of inhabitants of a certain age class (he_x_y) \n",
    "* Scales the values into range [0,1] or [-1,1]\n",
    "* Data should not have any large outliers (data exploration!), as the rest of the data will be squashed into narrow range. -> Standardization is better option\n",
    "* Scikit-learn: [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "## 7.2 Standardization or Z-score normalization\n",
    "\n",
    "* X_new = (X - mean)/std\n",
    "* Used when \"zero mean and unit standard deviation\" needs to be ensured, we are standardizing to achieve equal variance of features\n",
    "* Not bound to specific range\n",
    "* less affected by outliers, as range is not set outliers will not have influence on the range of other values\n",
    "* \"1 implies that the value for that case is one standard deviation above the mean\"\n",
    "* Scikit-learn: [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all column headings\n",
    "all_columns = list(x_train.columns)\n",
    "\n",
    "col_names_no_scaling.extend(['postinumer'])\n",
    "print(col_names_no_scaling)\n",
    "\n",
    "# List of column names we want to scale. (all columns minus those we don't want)\n",
    "col_names_to_scale = [column for column in all_columns if column not in col_names_no_scaling]\n",
    "\n",
    "# Subset the data for only those to-be scaled\n",
    "x_train_to_scale = x_train[col_names_to_scale]\n",
    "# we do not need to scale the label, but we also need to scale the test and validation data\n",
    "x_test_to_scale = x_test[col_names_to_scale]\n",
    "x_val_to_scale = x_val[col_names_to_scale]\n",
    "\n",
    "\n",
    "# Apply a Scikit StandardScaler() or MinMaxScaler() for all the columns left in dataframe\n",
    "# You can also test both, rename variable `train/test/val_dataset_name` after running the remaining cells with one scaler, to not overwrite results\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "# You can save the scaler for later use. If there suddenly would be more zip codes in Finland, we should use the same scaler.\n",
    "# dump(scaler, scaler_path, compress=True)\n",
    "\n",
    "# We fit the scaler to the training dataset and transform the trainin dataset\n",
    "scaled_x_train_array = scaler.fit_transform(x_train_to_scale)\n",
    "\n",
    "# we also need to scale x_test and x_val with the same scaler, note that we only transform , not fit the test data\n",
    "scaled_x_test_array = scaler.transform(x_test_to_scale)\n",
    "scaled_x_val_array = scaler.transform(x_val_to_scale)\n",
    "\n",
    "# Result is a numpy ndarray, which we pack back into geopandas dataframe\n",
    "# Join the non-scaled columns back with the the scaled columns by index and drop all rows that have nodata values after scaling\n",
    "def to_pandas_and_rejoin(scaled_array, col_names_to_scale, unscaled_data):\n",
    "    scaled_x = pd.DataFrame(scaled_array)\n",
    "    scaled_x.columns = col_names_to_scale\n",
    "    full_scaled_x = scaled_x.join(unscaled_data).dropna()\n",
    "    return full_scaled_x\n",
    "    \n",
    "\n",
    "full_scaled_x_train = to_pandas_and_rejoin(scaled_x_train_array, col_names_to_scale, x_train[col_names_no_scaling])\n",
    "full_scaled_x_test = to_pandas_and_rejoin(scaled_x_test_array, col_names_to_scale, x_test[col_names_no_scaling])\n",
    "full_scaled_x_val = to_pandas_and_rejoin(scaled_x_val_array, col_names_to_scale, x_val[col_names_no_scaling])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to visualize the resulting datasets on a map, you can join the geometry back to the zip code and store as geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_gdf[['postinumer','geometry']].merge(x_train,on='postinumer', how='right').to_file(train_dataset_geo, driver='GPKG')  \n",
    "original_gdf[['postinumer','geometry']].merge(x_val,on='postinumer', how='right').to_file(val_dataset_geo, driver='GPKG')  \n",
    "original_gdf[['postinumer','geometry']].merge(x_test,on='postinumer', how='right').to_file(test_dataset_geo, driver='GPKG') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the results without geometries of this notebook in two further notebooks, so we will store the prepared train, validation and test datasets without geometries and zip codes into csv.\n",
    "We also store the labels for train, validation and test datasets as pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prepared train and test zipcode datasets to csv, drop the zip code column ('postinumer') for that\n",
    "full_scaled_x_train.drop(['postinumer'], axis=1).to_csv(scaled_train_dataset_name, index=False)\n",
    "full_scaled_x_test.drop(['postinumer'], axis=1).to_csv(scaled_test_dataset_name, index=False)\n",
    "full_scaled_x_val.drop(['postinumer'], axis=1).to_csv(scaled_val_dataset_name, index=False)\n",
    "\n",
    "# You can also store the unscaled train, test and validation datasets, which can be used with tree-based models\n",
    "x_train.drop(['postinumer'], axis=1).to_csv(train_dataset_name, index=False)\n",
    "x_test.drop(['postinumer'], axis=1).to_csv(test_dataset_name, index=False)\n",
    "x_val.drop(['postinumer'], axis=1).to_csv(val_dataset_name, index=False)\n",
    "\n",
    "# Write the labels to pickle, as we do not need to read it outside of these notebooks, otherwise json or csv would be more compatible options\n",
    "y_train.to_pickle(train_label_name)\n",
    "y_test.to_pickle(test_label_name)\n",
    "y_val.to_pickle(val_label_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
