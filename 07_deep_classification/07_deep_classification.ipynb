{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c841c3-ef81-46d5-a0fe-5512104cb4ae",
   "metadata": {},
   "source": [
    "# Exercise 7. Classification, deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a0786-7f56-447f-80d5-fca156091cfa",
   "metadata": {},
   "source": [
    "The aim of this exercise is to train a deep learning model for predicting different classes from satellite data. It also assesses the model accuracy with a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0322a88-c1a8-44e1-a07b-5c6c7cf29188",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "2 raster files with:\n",
    "\n",
    "* Coordinate system: Finnish ETRS-TM35FIN, EPSG:3067\n",
    "* Resolution: 20m\n",
    "* BBOX: 200000, 6700000, 300000, 6800000\n",
    "\n",
    "#### Labels\n",
    "\n",
    "* Multiclass classification raster: 1 - forest, 2 - fields, 3 - water, 4 - urban, 0 - everything else.\n",
    "\n",
    "#### Data image\n",
    "\n",
    "* Sentinel2 mosaic, with data from 2 different dates (May and July), to have more data values. Dataset has 8 bands based on bands: 2, 3, 4 and 8 on dates: 2021-05-11 and 2021-07-21, reflection values scaled to [0 ... 1]. The bands source data is: \n",
    "     *  'b02' / '2021-05-11'\n",
    "     *  'b02' / '2021-07-21'\n",
    "     *  'b03' / '2021-05-11'\n",
    "     *  'b03' / '2021-07-21'\n",
    "     *  'b04' / '2021-05-11'\n",
    "     *  'b04' / '2021-07-21'\n",
    "     *  'b08' / '2021-05-11'\n",
    "     *  'b08' / '2021-07-21'\n",
    "     \n",
    "[Bands](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/bands/): b02=blue, b03=green, b04=red, b08=infrared          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6dd87-57c4-460a-a941-b2c4f79ea080",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Trained deep learning model\n",
    "* Model accuracy estimation\n",
    "* Class confusion matrix\n",
    "* Predicted image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d602d4-bf72-43ce-8486-2c3a0174bb96",
   "metadata": {},
   "source": [
    "## Main steps\n",
    "\n",
    "1) Read data and shape it to suitable form for scikit-learn.\n",
    "2) Divide the data to training, validation and test datasets.\n",
    "3) Undersample to balance the training dataset.\n",
    "4) Train the model.\n",
    "5) Estimate the model on test data, inc class confusion matrix classification report creation.\n",
    "6) Predict classification based on the data image and save it.\n",
    "7) Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ad20e-88f3-4930-b153-987163aa7d25",
   "metadata": {},
   "source": [
    "## Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5bca4-9244-4ad0-8053-df39d5720f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import urllib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6525082-3657-4938-813e-06d71fe6eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File paths.\n",
    "# Source data URLs\n",
    "image_url = 'https://a3s.fi/gis-courses/gis_ml/image.tif'\n",
    "multiclass_classification_url = 'https://a3s.fi/gis-courses/gis_ml/labels_multiclass.tif'\n",
    "\n",
    "# Folders\n",
    "user = os.environ.get('USER')\n",
    "base_folder = os.path.join('/scratch/project_2002044', user, '2022/GeoML')\n",
    "dataFolder = os.path.join(base_folder,'data')\n",
    "outputBaseFolder= os.path.join(base_folder,'07_deep_classification')\n",
    "shallow_folder= os.path.join(base_folder,'05_shallow_classification')\n",
    "\n",
    "# Source data local paths\n",
    "image_file = os.path.join(dataFolder, 'image.tif')\n",
    "multiclass_classification_file = os.path.join(dataFolder, 'labels_multiclass.tif')\n",
    "\n",
    "# Outputs of the model\n",
    "# Saved model and its weights\n",
    "fullyConnectedModel = os.path.join(outputBaseFolder,'fullyConnectedModel.json')\n",
    "fullyConnectedWeights = os.path.join(outputBaseFolder,'fullyConnectedWeights.h5')\n",
    "# Predicted .tif image\n",
    "predictedImageFile = os.path.join(outputBaseFolder,'classified_fullyConnected.tif')\n",
    "\n",
    "#For comparision\n",
    "random_forest_predicition = os.path.join(shallow_folder,'classification_random_forest.tif')\n",
    "SGD_predicition = os.path.join(shallow_folder,'classification_SGD.tif')\n",
    "gradient_boost_predicition = os.path.join(shallow_folder,'classification_gradient_boost.tif')\n",
    "\n",
    "# BBOX for exercise data, we use less than full image for shallow learning training, because of speed and to better see the results when plotting.\n",
    "minx = 240500\n",
    "miny = 6775500\n",
    "maxx = 253500\n",
    "maxy = 6788500 \n",
    "\n",
    "# Available cores. During the course only 1 core is available, outside of this course more cores might be available \n",
    "# You can make use of multiple cores by setting this number to the number of cores available.\n",
    "n_jobs = 1\n",
    "\n",
    "# During the course we run this on CPU, but all bigger deep learning models benefit from running on GPU.\n",
    "# No changes to code should be needed to run this on GPU.\n",
    "\n",
    "From second code box: # Available cores. During the course only 1 core is available, otherwise more cores could be available, so increase this then. outside of this course more cores might be available ; you can make use of multiple cores by setting this number to the number of cores available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c81b17-cad0-4c04-8261-5ea547f332c0",
   "metadata": {},
   "source": [
    "(Download input data if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1994c-0eb5-4262-a85b-37e893a510f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(dataFolder):\n",
    "    os.makedirs(dataFolder)\n",
    "    \n",
    "if not os.path.exists(image_file):\n",
    "    urllib.request.urlretrieve(image_url, image_file)\n",
    "    \n",
    "if not os.path.exists(multiclass_classification_file):\n",
    "    urllib.request.urlretrieve(multiclass_classification_url, multiclass_classification_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56229152-9a7b-411e-94ad-0802f581a5a9",
   "metadata": {},
   "source": [
    "## Read data and shape it to suitable form for scikit-learnÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b667115-8404-4c5e-abf3-959dba1ce23f",
   "metadata": {},
   "source": [
    "Read the input datasets with Rasterio and shape it to suitable form for keras (same as for scikit-learn).\n",
    "\n",
    "Exactly the same as for clustering or shallow classification data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414b33e-9b40-48e9-b298-d4e4f481c668",
   "metadata": {},
   "source": [
    "### Satellite image\n",
    "\n",
    "The satellite image has 8 channels, so rasterio reads it in as 3D data cube.\n",
    "\n",
    "For keras we reshape the data to 2D, having in dataframe one row for each pixel. Each pixel has eight values, one for each band/date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963daee-8d82-42cd-8839-2954e658747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pixel values from .tif file as dataframe\n",
    "with rasterio.open(image_file) as image_dataset:\n",
    "    image_data = image_dataset.read(window=from_bounds(minx, miny, maxx, maxy, image_dataset.transform)) \n",
    "\n",
    "# Check shape of input data\n",
    "print ('Dataframe original shape, 3D: ', image_data.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5526d-208a-484f-afdb-d34d4f02f648",
   "metadata": {},
   "source": [
    "Save number of bands for later, to be able to reshape data back to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e616b-0f14-4c9d-b397-ac2568ac30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bands_in_image = image_data.shape[0]\n",
    "no_bands_in_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c588dce-444f-47b7-b956-9cef59f4e334",
   "metadata": {},
   "source": [
    "As a mid-step transponse the axis order, so that the bands are the last. Notice how the dataframe size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fb1db-6faa-4b54-bfb4-c17b2f7e47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data2 = np.transpose(image_data, (1, 2, 0))\n",
    "# Check again the data shape, now the bands should be last.\n",
    "print ('Dataframe shape after transpose, 3D: ', image_data2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3426af-6fa1-4128-82a6-0d1224380121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then reshape to 2D.\n",
    "pixels = image_data2.reshape(-1, no_bands_in_image)\n",
    "print ('Dataframe shape after transpose and reshape, 2D: ', pixels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fa8f1-8774-4f79-bdc2-88df63180b1b",
   "metadata": {},
   "source": [
    "### Forest classes image as labels\n",
    "\n",
    "Do the same for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4f7f7-db40-4a24-83eb-dc1658b108ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labels only reshape to 1D is enough.\n",
    "with rasterio.open(multiclass_classification_file) as src:\n",
    "    labels_data = src.read(window=from_bounds(minx, miny, maxx, maxy, src.transform))\n",
    "    input_labels = labels_data.reshape(-1)\n",
    "    print ('Labels shape after reshape, 1D: ', input_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4c4c2-81dd-427e-9057-ace2d97d36a7",
   "metadata": {},
   "source": [
    "Save the number of classes in labels, it will be later needed for defining the last layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0ff22-eab3-449a-9d87-f2d03635dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = np.unique(labels_data).size\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ce3aa-1911-4035-8790-9be2263a432b",
   "metadata": {},
   "source": [
    "### Divide the data to training, validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3477ad9-d64a-428e-b272-73e6218da3f2",
   "metadata": {},
   "source": [
    "Set training, validation and test data ratios, how big part of the pixels is assigned to different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63612eb-cf3f-4d9f-be24-f90bd156a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdba6ed-950e-492a-b56d-5c0dfc256b97",
   "metadata": {},
   "source": [
    "First separate test set. (In the exercise we will not use test data, but in actual projects you should.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43446410-9974-4521-b7f0-68da91e70d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rest, x_test, y_rest, y_test = train_test_split(pixels, input_labels, test_size=test_ratio, random_state=63, stratify=input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ab3ed-e9fc-4dae-80f4-4917fe9b2790",
   "metadata": {},
   "source": [
    "... and then training and validation set, using the ratios set above and keeping class representation the same in all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c9e33-d4b5-4db5-b25b-633b8744419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_validation, y_train1, y_validation= train_test_split(x_rest, y_rest, test_size=validation_ratio/(train_ratio + validation_ratio), random_state=63, stratify=y_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fe083-dcbc-4b22-b240-89a4ef808c9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resample to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07eb370-543a-4c10-8c51-7ea2adc8bcfd",
   "metadata": {},
   "source": [
    "The classes are very imbalanced in the dataset, so undersample the majority classes in the training set, so that all classes are represented about similar number of pixels. \n",
    "Notice that validation and test set keep the original class-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04102368-3cdc-430d-bb1e-ff3cf186e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9257d81-2b34-43b3-bd2f-d2614d0b7467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are very imbalanced, so undersample the majority classes\n",
    "rus = RandomUnderSampler(random_state=63)\n",
    "x_train, y_train = rus.fit_resample(x_train1, y_train1)   \n",
    "print ('Dataframe shape after undersampling of majority classes, pixels 2D: ', x_train.shape)\n",
    "print ('Dataframe shape after undersampling of majority classes, labels 2D: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954884f-104e-4732-8448-ecb55e591633",
   "metadata": {},
   "source": [
    "*How many pixels of different classes are included in training dataset?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a1b79-46d6-4955-9ca3-b2a1c8401f66",
   "metadata": {},
   "source": [
    "Notice that we lost a lot of pixel at this point, in real cases that may be undesired. See [inbalanced-learn User guide](https://imbalanced-learn.org/stable/user_guide.html#user-guide) for other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b29dc0-8da4-4722-bcfc-dcd68ca9fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels before splitting:           ', np.unique(input_labels, return_counts=True)[1])\n",
    "print('Training data before undersampling:', np.unique(y_train1, return_counts=True)[1])\n",
    "print('Training data after undersampling: ', np.unique(y_train, return_counts=True)[1])\n",
    "print('Validation data:                   ', np.unique(y_validation, return_counts=True)[1])\n",
    "print('Test data:                         ', np.unique(y_test, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2270213-7b14-4be8-857a-649f781830f3",
   "metadata": {},
   "source": [
    "## Define and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84011c-2c84-433d-950a-11e5efcd8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a sequential model\n",
    "model = models.Sequential()\n",
    "# adding the first layer containing 64 perceptrons. 3 is representing the number of bands used for training\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(no_bands_in_image,)))\n",
    "# add the first dropout layer\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "# adding more layers to the model\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "# for last layer, the activation is 'softmax', it should be that for multi-class classification models\n",
    "model.add(layers.Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe512e-4bbf-457c-bfd8-dd5112ca5cc0",
   "metadata": {},
   "source": [
    "Compile the model, using:\n",
    " - `Adam optimizer`, often used, but could be some other optimizer too.\n",
    " - Some other learning rate could be tried\n",
    " - `categorical_crossentropy` loss function (should be used with multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e19462-dd34-476d-8566-08b0c223ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b70bf9-9b50-4081-8857-a9a0be4a1908",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c760d53-0161-4830-a759-eadaa30b9cae",
   "metadata": {},
   "source": [
    "Encode the labels categorically (as we did with the region names in Postcode preparations). As result each pixel has a label, which is a 1D vector with 5 elements, each representing the probability of belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755cbfa-e40c-45b4-9688-6bdf32777de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = to_categorical(y_train)\n",
    "y_train_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f69ae-d29d-45eb-b67f-b785441e898f",
   "metadata": {},
   "source": [
    "Train the model and save it. *This takes a moment, please wait*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eca582-e4d0-4c4e-b214-47a9422635cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "model.fit(x_train, y_train_categorical, epochs=500, batch_size=256, verbose=2)\n",
    "\n",
    "# Save the model to disk\n",
    "# Serialize the model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(fullyConnectedModel, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(fullyConnectedWeights)\n",
    "print('Saved model to disk:  \\nModel: ', fullyConnectedModel, '\\nWeights: ',  fullyConnectedWeights)\n",
    "print('Model training took: ', round((time.time() - start_time), 0), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95549b09-d333-473c-a04b-9c531bc88507",
   "metadata": {},
   "source": [
    "### Estimate the model on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dab82-464a-4754-a039-cac64ae6f88d",
   "metadata": {},
   "source": [
    "Find accuracy using Keras own `evaluate()`-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2056ab-f3e6-42d2-a505-d6d1c8b72d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_categorical = to_categorical(y_validation)\n",
    "\n",
    "# Use verbose=0 when using this in batch jobs, avoids printing to output a lot of unclear text.\n",
    "validation_loss, validation_acc = model.evaluate(x_validation, y_validation_categorical, verbose=1)\n",
    "print('Validation accuracy:', validation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0581b5-3f1e-4454-8446-2cac78f473a9",
   "metadata": {},
   "source": [
    "Calculate confusion matrix and classification report as we did with shallow classifier. Use `scikit-learn` functions for that.\n",
    "\n",
    "First predict for the x_validation. The model returns a 2D array, with:\n",
    "- each row representing one pixel.\n",
    "- each column representing the probablity of this pixel representing each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311f495-6939-44e3-8f88-cecd06ffa42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction = model.predict(x_validation)\t\n",
    "print ('Validation prediction dataframe shape, original 2D: ', validation_prediction.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516560b-b5c7-4e03-83ce-1cb902c417e0",
   "metadata": {},
   "source": [
    "Find which class was most likely for each pixel and select only that class for the output. Output is 1D array, with the most likely class index given for each pixel. `Argmax` returns the indices of the maximum values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde68f4-95c1-4416-af00-1e42448e299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(validation_prediction,axis=1)\n",
    "print ('Validation prediction dataframe shape, after argmax, 1D: ', predicted_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a25007-2acd-4401-9c6f-c8ab05a256db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix: \\n', confusion_matrix(y_validation, predicted_classes))\n",
    "print('Classification report: \\n', classification_report(y_validation, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1480f5c-6f2d-4aec-a8ac-a7f69131b085",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  Skipped here, but in real case, you should run similar evaluation also with test dataset after finilizing your model, optimizer, loss etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467804f-bc45-45f3-823a-0b7eda47b063",
   "metadata": {},
   "source": [
    "## Predict classification based on the data image and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d12b0-b740-4bd0-87d1-3876c314f490",
   "metadata": {},
   "source": [
    "Very similar to the shallow classifiers, but:\n",
    " - `argmax` is used for finding the most likely class.\n",
    " - Data type is changed to int8, keras returns int64, which GDAL does not support.   \n",
    " \n",
    " Load the model from .json file and re-create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c448a4-4f68-49e9-bddc-e4e3c9b6f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(fullyConnectedModel, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into the model\n",
    "loaded_model.load_weights(fullyConnectedWeights)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558e9bf-a7f8-49e2-9f87-872a0363db36",
   "metadata": {},
   "source": [
    "Predict for all pixels, reshape data back to image and save it as file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbc86e-74ed-4154-b8bc-d05fcca2457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "# Predict for all pixels\n",
    "prediction = loaded_model.predict(pixels)\n",
    "print ('Prediction dataframe shape, original 2D: ', prediction.shape)\n",
    "\n",
    "# Find the most likely class for each pixel.\n",
    "predicted_classes = np.argmax(prediction,axis=1)\n",
    "print ('Prediction dataframe shape, after argmax, 1D: ', predicted_classes.shape)\n",
    "\n",
    "# Reshape back to 2D as in original raster image\n",
    "prediction2D = np.reshape(predicted_classes, (image_data.shape[1], image_data.shape[2]))\n",
    "print('Prediction shape in 2D: ', prediction2D.shape)\n",
    "\n",
    "# Change data type to int8\n",
    "predicted2D_int8 = np.int8(prediction2D)\n",
    "\n",
    "# Save the results as .tif file.\n",
    "# Copy the coordinate system information, image size and other metadata from the satellite image \n",
    "outputMeta = image_dataset.meta\n",
    "# Change the number of bands and data type.\n",
    "outputMeta.update(count=1, dtype='int8', nodata=100)\n",
    "# Writing the image on the disk\n",
    "with rasterio.open(predictedImageFile, 'w', **outputMeta) as dst:\n",
    "    dst.write(predicted2D_int8, 1)\n",
    "\n",
    "print('Predicting took: ', round((time.time() - start_time), 0), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dabbc6-abca-44d8-a2f2-2b8f737c111e",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556b6e6-8e52-4ce2-9091-427ff6537f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Help function to normalize band values and enhance contrast. Just like what QGIS does automatically\n",
    "def normalize(array):\n",
    "    min_percent = 2   # Low percentile\n",
    "    max_percent = 98  # High percentile\n",
    "    lo, hi = np.percentile(array, (min_percent, max_percent))\n",
    "    return (array - lo) / (hi - lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5088-9438-4067-b4e9-c7f0e8cd5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a subplot for 4 images and plot the sentinel image \n",
    "fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(10, 15))\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"white\",\"green\",\"orange\",\"blue\",\"violet\"])\n",
    "\n",
    "### The results\n",
    "rf_results = rasterio.open(random_forest_predicition)\n",
    "show(rf_results, ax=ax[0, 0], cmap=cmap, title='Random forest')\n",
    "\n",
    "SGD_results = rasterio.open(SGD_predicition)\n",
    "show(SGD_results, ax=ax[0, 1], cmap=cmap, title='SGD')\n",
    "\n",
    "gradient_boost_results = rasterio.open(gradient_boost_predicition)\n",
    "show(gradient_boost_results, ax=ax[2, 0], cmap=cmap, title='gradient_boost')\n",
    "\n",
    "show(predicted2D_int8, ax=ax[2, 1], cmap=cmap, title='Dense deep network')\n",
    "\n",
    "# Plot the sentinel image \n",
    "nir, red, green = image_data[7,], image_data[3,], image_data[1,]\n",
    "nirn, redn, greenn = normalize(nir), normalize(red), normalize(green)\n",
    "stacked = np.stack((nirn, redn, greenn))\n",
    "show(stacked, ax=ax[1,0], title='image') \n",
    "\n",
    "#labels = rasterio.open(labelsImage)\n",
    "show(labels_data, ax=ax[1,1], cmap=cmap, title='labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007211e-ae22-494d-a766-4afb4bc17eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
