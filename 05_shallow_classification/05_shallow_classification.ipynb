{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5. Classification, shallow learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to train 4 different shallow learning models to predict different land-use classes from satellite data. It also assesses the model accuracy with a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "2 raster files with:\n",
    "\n",
    "* Coordinate system: Finnish ETRS-TM35FIN, EPSG:3067\n",
    "* Resolution: 20m\n",
    "* BBOX: 200000, 6700000, 300000, 6800000\n",
    "\n",
    "#### Labels\n",
    "\n",
    "* Multiclass classification raster: 1 - forest, 2 - fields, 3 - water, 4 - urban, 0 - everything else.\n",
    "\n",
    "#### Data image\n",
    "\n",
    "* Sentinel2 mosaic, with data from 2 different dates (May and July), to have more data values. Dataset has 8 bands based on bands: 2, 3, 4 and 8 on dates: 2021-05-11 and 2021-07-21, reflection values scaled to [0 ... 1]. The bands source data is: \n",
    "     *  'b02' / '2021-05-11'\n",
    "     *  'b02' / '2021-07-21'\n",
    "     *  'b03' / '2021-05-11'\n",
    "     *  'b03' / '2021-07-21'\n",
    "     *  'b04' / '2021-05-11'\n",
    "     *  'b04' / '2021-07-21'\n",
    "     *  'b08' / '2021-05-11'\n",
    "     *  'b08' / '2021-07-21'\n",
    "     \n",
    "[Bands](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/bands/): b02=blue, b03=green, b04=red, b08=infrared     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The trained models: \n",
    "* Random forest\n",
    "* Stochastic Gradient Decent\n",
    "* Gradient Boost\n",
    "* SVM Suppot Vector Classifier\n",
    "\n",
    "For each model:\n",
    "* Trained model\n",
    "* Model accuracy estimation\n",
    "* Class confusion matrix\n",
    "* Predicted image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps\n",
    "\n",
    "1) Read data and shape it to suitable form for scikit-learn.\n",
    "2) Divide the data to training, validation and test datasets.\n",
    "3) Undersample to balance the training dataset.\n",
    "4) For each model:\n",
    "   * Train the model.\n",
    "   * Estimate the model on test data, inc class confusion matrix classification report creation.\n",
    "   * Predict classification based on the data image and save it.\n",
    "5) For SVM use grid search to find optimal settings.\n",
    "6) Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import urllib\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.windows import from_bounds\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File paths.\n",
    "# Source data URLs\n",
    "image_url = 'https://a3s.fi/gis-courses/gis_ml/image.tif'\n",
    "multiclass_classification_url = 'https://a3s.fi/gis-courses/gis_ml/labels_multiclass.tif'\n",
    "\n",
    "# Folders\n",
    "user = os.environ.get('USER')\n",
    "base_folder = os.path.join('/scratch/project_2002044', user, '2022/GeoML')\n",
    "dataFolder = os.path.join(base_folder,'data')\n",
    "outputBaseFolder= os.path.join(base_folder,'05_shallow_classification')\n",
    "\n",
    "# Source data local paths\n",
    "image_file = os.path.join(dataFolder, 'image.tif')\n",
    "multiclass_classification_file = os.path.join(dataFolder, 'labels_multiclass.tif')\n",
    "\n",
    "# BBOX for exercise data, we use less than full image for shallow learning training, because of speed and to better see the results when plotting.\n",
    "minx = 240500\n",
    "miny = 6775500\n",
    "maxx = 253500\n",
    "maxy = 6788500 \n",
    "\n",
    "# Available cores. During the course only 1 core is available, outside of this course more cores might be available \n",
    "# You can make use of multiple cores by setting this number to the number of cores available.\n",
    "n_jobs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Download input data if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(dataFolder):\n",
    "    os.makedirs(dataFolder)\n",
    "\n",
    "if not os.path.exists(image_file):\n",
    "    urllib.request.urlretrieve(image_url, image_file)\n",
    "    \n",
    "if not os.path.exists(multiclass_classification_file):\n",
    "    urllib.request.urlretrieve(multiclass_classification_url, multiclass_classification_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Read data and shape it to suitable form for scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the input datasets with Rasterio and shape it to suitable form for scikit-learn.\n",
    "\n",
    "Exactly the same as for K-means for image data, the similar processing only added for the labels image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite image\n",
    "\n",
    "The satellite image has 8 channels, so rasterio reads it in as 3D data cube.\n",
    "\n",
    "For scikit-learn we reshape the data to 2D, having in dataframe one row for each pixel. Each pixel has eight values, one for each band/date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pixel values from .tif file as dataframe\n",
    "with rasterio.open(image_file) as image_dataset:\n",
    "    image_data = image_dataset.read(window=from_bounds(minx, miny, maxx, maxy, image_dataset.transform))\n",
    "\n",
    "# Check shape of input data\n",
    "print ('Dataframe original shape, 3D: ', image_data.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save number of bands for later, to be able to reshape data back to 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bands_in_image = image_data.shape[0]\n",
    "no_bands_in_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a mid-step transponse the axis order, so that the bands are the last. Notice how the dataframe size changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data2 = np.transpose(image_data, (1, 2, 0))\n",
    "# Check again the data shape, now the bands should be last.\n",
    "print ('Dataframe shape after transpose, 3D: ', image_data2.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then reshape to 2D.\n",
    "pixels = image_data2.reshape(-1, no_bands_in_image)\n",
    "print ('Dataframe shape after transpose and reshape, 2D: ', pixels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "Do the same for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labels only reshape to 1D is enough.\n",
    "with rasterio.open(multiclass_classification_file) as labels_src:\n",
    "    labels_data = labels_src.read(window=from_bounds(minx, miny, maxx, maxy, labels_src.transform))\n",
    "    input_labels = labels_data.reshape(-1)\n",
    "    print ('Labels shape after reshape, 1D: ', input_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that labels data has only one band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Divide the data to training, validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set training, validation and test data ratios, how big part of the pixels is assigned to different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rest, x_test, y_rest, y_test = train_test_split(pixels, input_labels, test_size=test_ratio, random_state=63, stratify=input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then training and validation set, using the ratios set above and keeping class representation the same in all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_validation, y_train1, y_validation= train_test_split(x_rest, y_rest, test_size=validation_ratio/(train_ratio + validation_ratio), random_state=63, stratify=y_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3 Resample to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are very imbalanced in the dataset, so undersample the majority classes in the training set, so that all classes are represented about similar number of pixels. \n",
    "Notice that validation and test set keep the original class-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = show_hist(labels_data, label='Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=63)\n",
    "x_train, y_train = rus.fit_resample(x_train1, y_train1)   \n",
    "print ('Dataframe shape after undersampling of majority classes, 2D: ', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many pixels of different classes are included in training dataset?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we lost a lot of pixel at this point, in real cases that may be undesired. See [inbalanced-learn User guide](https://imbalanced-learn.org/stable/user_guide.html#user-guide) for other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels before splitting:           ', np.unique(input_labels, return_counts=True)[1])\n",
    "print('Training data before undersampling:', np.unique(y_train1, return_counts=True)[1])\n",
    "print('Training data after undersampling: ', np.unique(y_train, return_counts=True)[1])\n",
    "print('Validation data:                   ', np.unique(y_validation, return_counts=True)[1])\n",
    "print('Test data:                         ', np.unique(y_test, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Modelling\n",
    "### Funcitons for training and estimating the models and predicting based on the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar functions will be used by different algorithms. Here the functions are only defined, they will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x_train, y_train, clf, classifierName):\n",
    "    start_time = time.time()    \n",
    "    clf.fit(x_train, y_train)\n",
    "    print('Model training took: ', round((time.time() - start_time), 2), ' seconds')\n",
    "    \n",
    "    # Save the model to a file\n",
    "    modelFilePath = os.path.join(outputBaseFolder, ('model_' + classifierName + '.sav'))\n",
    "    dump(clf, modelFilePath) \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model may be estimated first with validation data and then with test data. Both confusion matrix and classification report are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateModel(clf, x_test, y_test):\n",
    "    test_predictions = clf.predict(x_test)\n",
    "    print('Confusion matrix: \\n', confusion_matrix(y_test, test_predictions))\n",
    "    print('Classification report: \\n', classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classification based on the data image and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage(modelName):\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    #Set file paths\n",
    "    classifiedImageFile = os.path.join(outputBaseFolder, ('classification_' + modelName + '.tif'))\n",
    "    modelFile = os.path.join(outputBaseFolder, ('model_' + modelName + '.sav'))    \n",
    "         \n",
    "    #Load the model from the saved file\n",
    "    trained_model = load(modelFile)\n",
    "\n",
    "    # predict the class for each pixel\n",
    "    prediction = trained_model.predict(pixels)\n",
    "\n",
    "    # Reshape back to 2D\n",
    "    print('Prediction shape in 1D: ', prediction.shape)\n",
    "    prediction2D = np.reshape(prediction, (image_data.shape[1], image_data.shape[2]))\n",
    "    print('Prediction shape in 2D: ', prediction2D.shape)\n",
    "\n",
    "    # Save the results as .tif file.\n",
    "    # Copy metadata from the labels image \n",
    "    outputMeta = labels_src.meta\n",
    "    # Writing the image on the disk\n",
    "    with rasterio.open(classifiedImageFile, 'w', **outputMeta) as dst:\n",
    "        dst.write(prediction2D, 1)\n",
    "    print('Predicting took: ', round((time.time() - start_time), 1), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierName = 'random_forest'\n",
    "# Initialize the random forest classifier and give the hyperparameters.\n",
    "clf_random_forest = RandomForestClassifier(n_estimators=200, max_depth=75, random_state=0, n_jobs=n_jobs)\n",
    "clf_random_forest = trainModel(x_train, y_train, clf_random_forest, classifierName)\n",
    "estimateModel(clf_random_forest, x_validation, y_validation) #Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feel free, to modify some some of the hyper-parameters above to get better results.*\n",
    "And then see with test data, if the modifications help also for previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateModel(clf_random_forest, x_test, y_test) #Test data\n",
    "predictImage(classifierName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature importances:')\n",
    "classnames = ['blue-may', 'blue-july', 'green-may', 'green-july', 'red-may', 'red-july', 'infrared-may', 'infrared-july']\n",
    "for importance in list(zip(classnames,clf_random_forest.feature_importances_)):\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierName = 'SGD'    \n",
    "clf_SGD = SGDClassifier(loss=\"log_loss\", learning_rate='adaptive', eta0=.1, alpha=1e-5,  n_jobs=n_jobs, max_iter=2000, penalty='l1') #\n",
    "clf_SGD = trainModel(x_train, y_train, clf_SGD, classifierName)\n",
    "estimateModel(clf_SGD, x_validation, y_validation) #Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateModel(clf_SGD, x_test, y_test) #Test data\n",
    "predictImage(classifierName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierName = 'gradient_boosting'    \n",
    "clf_gradient_boosting = GradientBoostingClassifier(n_estimators=1000, learning_rate=.05)\n",
    "clf_gradient_boosting = trainModel(x_train, y_train, clf_gradient_boosting, classifierName)\n",
    "estimateModel(clf_gradient_boosting, x_validation, y_validation) #Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateModel(clf_gradient_boosting, x_test, y_test) #Test data\n",
    "predictImage(classifierName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature importances:')\n",
    "classnames = ['blue-may', 'blue-july', 'green-may', 'green-july', 'red-may', 'red-july', 'infrared-may', 'infrared-july']\n",
    "for importance in list(zip(classnames,clf_gradient_boosting.feature_importances_)):\n",
    "    print(importance)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVM is slower than others, wait a moment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierName = 'SVM'        \n",
    "clf_svc = SVC(kernel='rbf', gamma='auto',  decision_function_shape='ovr')\n",
    "clf_svc = trainModel(x_train, y_train, clf_svc, classifierName)\n",
    "estimateModel(clf_svc, x_validation, y_validation) #Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Grid Search for SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models have different settings (hyperparameters) that can be used for searching best model. Grid search is one option to automatically search for better option. For more options in hyperparameter search see [CSC machine learning guide](https://docs.csc.fi/support/tutorials/hyperparameter_search/)\n",
    "\n",
    "Here we try different `C` and `gamma` values for the SVM model. Grid search automatically saves the best model.\n",
    "\n",
    "*Notice, how the results are improved from the first SVM result above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierName = 'SVC_grid_search'        \n",
    "# Find the optimal parameters for SVM\n",
    "param_grid = {'C': [1000, 10000], 'gamma': [1, 10]}\n",
    "# Initialize the grid search, cv is the number of iterations, kept at minimum here for faster results.\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=1, n_jobs=n_jobs, cv=2)    \n",
    "# Try different options\n",
    "grid = trainModel(x_train, y_train, grid, classifierName)\n",
    "\n",
    "# Plot the best option\n",
    "print('Best selected parameters: ',format(grid.best_params_))\n",
    "print('Best estimator: ',format(grid.best_estimator_))\n",
    "\n",
    "# Test the classifier using test data\n",
    "estimateModel(grid, x_validation, y_validation) #Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateModel(grid, x_test, y_test) #Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Slow, wait.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImage(classifierName)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Help function to normalize band values and enhance contrast. Just like what QGIS does automatically\n",
    "def normalize(array):\n",
    "    min_percent = 2   # Low percentile\n",
    "    max_percent = 98  # High percentile\n",
    "    lo, hi = np.percentile(array, (min_percent, max_percent))\n",
    "    return (array - lo) / (hi - lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot for 6 images: 4 classification, 1 data image and 1 training labels. \n",
    "fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(10,15))\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"white\",\"green\",\"orange\",\"blue\",\"violet\"])\n",
    "\n",
    "# The prediction results\n",
    "rf_results = rasterio.open(os.path.join(outputBaseFolder,'classification_random_forest.tif'))\n",
    "show(rf_results, ax=ax[0, 0], cmap=cmap, title='Random forest')\n",
    "\n",
    "SGD_results = rasterio.open(os.path.join(outputBaseFolder,'classification_SGD.tif'))\n",
    "show(SGD_results, ax=ax[0, 1], cmap=cmap, title='SGD')\n",
    "\n",
    "gradient_boost_results = rasterio.open(os.path.join(outputBaseFolder,'classification_gradient_boosting.tif'))\n",
    "show(gradient_boost_results, ax=ax[2, 0], cmap=cmap, title='gradient_boost')\n",
    "\n",
    "SVM_grid_search_results = rasterio.open(os.path.join(outputBaseFolder,'classification_SVC_grid_search.tif'))\n",
    "show(SVM_grid_search_results, ax=ax[2, 1], cmap=cmap, title='SVM grid search')\n",
    "\n",
    "# Plot the sentinel image \n",
    "nir, red, green = image_data[7,], image_data[3,], image_data[1,]\n",
    "nirn, redn, greenn = normalize(nir), normalize(red), normalize(green)\n",
    "stacked = np.stack((nirn, redn, greenn))\n",
    "show(stacked, ax=ax[1,0], title='image') \n",
    "\n",
    "# Labels \n",
    "show(labels_data, ax=ax[1,1], cmap=cmap, title='labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
