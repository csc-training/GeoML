{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "TODO:\n",
    "\n",
    "* update data on Allas to newest versions and separate original data and intermediate resultsta\n",
    "* get original data from Puhti disk\n",
    "* fix comments\n",
    "* update filenames in paths cell\n",
    "* update links to short variable description to latest\n",
    "* Paths check, better to use absolute paths\n",
    "* if time: test using only x and y location \n",
    "* create spatial features\n",
    "* if time: check interactive map plotting\n",
    "* check spelling and grammar\n",
    "* use subplots for regression plots and find some nice variables to plot\n",
    "* correlation heatmap (leftovers) vs clusters vs PCA or similar\n",
    "* fill feature engineering\n",
    "* use scikits one hot encoder over dummys from pandas\n",
    "* check and sensify variable names\n",
    "* teminology check: variables/features/attributes\n",
    "* why store train and test sets as gpkg?\n",
    "* store scaled and unscaled data?\n",
    "* make sure to make it possible to go through in 30min\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector data for exercises\n",
    "\n",
    "In this course, we will use the vector dataset **Paavo**, which represents postal code area statistics collected by [Statistics Finland](https://www.stat.fi). Metadata description can be found on [Statistics Finland webpage](https://www.stat.fi/static/media/uploads/tup/paavo/paavo_kuvaus_en.pdf), see page 5 ff for field name descriptions.\n",
    "\n",
    "Short variable descriptions of this dataset can also be found here in Finnish and English\n",
    "* https://www.stat.fi/static/media/uploads/tup/paavo/paavo_lyhyt_kuvaus_2020_fi.pdf\n",
    "* https://www.stat.fi/static/media/uploads/tup/paavo/paavo_lyhyt_kuvaus_2020_en.pdf\n",
    "\n",
    "The dataset includes variables about each postcode area, describing:\n",
    "\n",
    "1. Population Structure (24 variables) HE\n",
    "2. Educational Structure (7 variables) KO\n",
    "3. Inhabitants' Disposable Monetary Income (7 variables) HR\n",
    "4. Size and Stage in Life of Households (15 variables) TE\n",
    "5. Households' Disposable Monetary Income (7 variables) TR\n",
    "6. Buildings and Dwellings (8 variables) RA\n",
    "7. Workplace Structure (26 variables) TP\n",
    "8. Main Type of Activity (9 variables) PT\n",
    "\n",
    "The overall goal of the exercises is to predict the median income for each zip code based on other variables/features of the dataset. \n",
    "This exercise is meant to show the different steps to prepare a vector dataset for machine learning. To make this task worth an exercise, all variables/features of type HR (that tell about the income) are removed from the dataset.\n",
    "\n",
    "Other spatial regression problems:\n",
    "* Predict age of tree based on height, circumference, location, species,...\n",
    "* predict algea concentration in lakes based on season, temperature, nutrients,...\n",
    "* ...\n",
    "\n",
    "## Vector data preparations\n",
    "\n",
    "Content of this notebook:\n",
    "0. Environment preparation\n",
    "1. Data retrieval\n",
    "2. Data exploration\n",
    "3. Feature selection\n",
    "4. Feature engineering\n",
    "5. Feature encoding\n",
    "6. Train/Test split\n",
    "7. Feature scaling\n",
    "8. Store the results\n",
    "\n",
    "In this notebook we will prepare the Paavo dataset for Machine Learning, by downloading all the necessary datasets, clean up some features, join auxiliary\n",
    "data and encode text fields, split the dataset into train and test set and scale the features for machine learning purposes. We will save the result as geopackage.\n",
    "\n",
    "The goal of this exercise is to get the dataset ready for subsequent machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment preparation\n",
    "\n",
    "Load all the needed Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# operating system level operations\n",
    "import os\n",
    "# for file operations\n",
    "import shutil\n",
    "# unpacking compressed files\n",
    "import zipfile\n",
    "# timing operations\n",
    "import time\n",
    "# data handling (and plotting)\n",
    "import pandas as pd\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# geospatial data handling \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon\n",
    "# Machine learning data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder \n",
    "# download data from URL\n",
    "from urllib.request import urlretrieve\n",
    "# for saving the scaler, uncomment following:\n",
    "# from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Data retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Creating directories\n",
    "Let's create a data directory in the base of our GeoML directory, where we store the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = {os.environ.get('USER')}\n",
    "base_directory= f'/scratch/project_2002044/{username}/GeoML'\n",
    "\n",
    "data_directory = os.path.join(base_directory,'original_data')\n",
    "\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Getting data\n",
    "Let's get the original datasets that we need for this exercise from Puhtis `data` (read only) directory.\n",
    "and unpack it into the just created directory. We could also remove the zip file after unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puhti_data_directory = '/data/geo/xxx'\n",
    "paavo = ''\n",
    "maakunta = ''\n",
    "\n",
    "shutil.copy(os.path.join(puhti_data_directory,paavo), data_directory)\n",
    "shutil.copy(os.path.join(puhti_data_directory,maakunta), data_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Reference for later: Getting data used in this notebook from Paituli**\n",
    "\n",
    "The following commands get you the same data that we have packaged for you in Allas from CSCs [Paituli spatial data download service](https://paituli.csc.fi).\n",
    "\n",
    "-> use wget (a command line tool, therefore we need the `!` when running it from within Jupyter, to run it in command line, remove the `!`). Note that you might have to create the `../original_data/vector/` directory first.\n",
    "\n",
    "To get the Paavo dataset:\n",
    "\n",
    "`!wget -r -l inf -N -np -nH -x -c --cut-dirs=4 ftp://ftp.funet.fi/index/geodata/tilastokeskus/paavo/2022/pno_tilasto_2022* -P ../original_data/vector/`\n",
    "\n",
    "where:\n",
    "-r, recursive download\n",
    "-l inf, how deep the recursive search goes, default is 5, here set to infinite\n",
    "-N, update only, do not download already existing files, this is important if download was interrupted or updating already existing data.\n",
    "-np, do not download parent directories\n",
    "-nH, remove hostname\n",
    "-x, make directories similarly to Paituli\n",
    "-cuts-dirs, cut certain number of directories from the beginning to avoid too deep directory trees\n",
    "-c, continue broken download\n",
    "\n",
    "Ftp stands for file transfer protocol and should be preferred when downloading files from the internet (if available)\n",
    "\n",
    "To get administrative borders:\n",
    "\n",
    "`!wget -r -l inf -N -np -nH -x -c --cut-dirs=4 ftp://ftp.funet.fi/index/geodata/mml/hallintorajat_10k/202_2022/SuomenMaakuntajako_2021_10k* -P ../original_data/vector/`\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Storing file locations\n",
    "\n",
    "In order to use the data later, we store their location (path) in variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_data_location = data_directory\n",
    "preprocessed_data_location = os.path.join(base_directory,'preprocessed_data')\n",
    "dataset_name = 'paavo' # to be changed to vector!\n",
    "\n",
    "zip_code_shapefile = os.path.join(original_data_location,dataset_name,'pno_tilasto_2020.shp')\n",
    "finnish_regions_shapefile = os.path.join(original_data_location,dataset_name,'SuomenMaakuntajako_2020_10k.shp')\n",
    "train_dataset_name = os.path.join(preprocessed_data_location,dataset_name,'train_zip_code_data.gpkg')\n",
    "test_dataset_name = os.path.join(preprocessed_data_location,dataset_name,'test_zip_code_data.gpkg')\n",
    "train_label_name = os.path.join(preprocessed_data_location,dataset_name,'train_income_labels.pkl')\n",
    "test_label_name = os.path.join(preprocessed_data_location,dataset_name,'test_income_labels.pkl')\n",
    "\n",
    "#scaler_path = '../original_data/paavo/zip_code_scaler.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration\n",
    "\n",
    "Always get to know your data before even thinking about Machine Learning. This section shows a few ways that we can get to know Paavo dataset a bit better. Possibilities are endless. For some models, you should also check that assumptions the model makes about data distribution are true.\n",
    "\n",
    "### 2.1 Read the data into dataframe\n",
    "\n",
    "Read the zip code dataset into a geopandas dataframe `original_gdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the encoding makes sure that characters are represented as intended, important especially with languages that have \"special characters\" \n",
    "original_gdf = gpd.read_file(zip_code_shapefile, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploring the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe columns and rows\n",
    "print(f\"Original dataframe size: {len(original_gdf.index)} rows (= zip codes) with {len(original_gdf.columns)} columns (=variables/features)\")\n",
    "# column names\n",
    "print(list(original_gdf.columns))\n",
    "# column data types\n",
    "# check for nodata\n",
    "# check for insensible cells \n",
    "original_gdf[original_gdf[\"hr_mtu\"]<=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization\n",
    "\n",
    "Another way of data exploration is visualizing different features of your dataset in different ways to reveal phenonemons that might not be visible when looking at numbers only.\n",
    "\n",
    "#### 2.3.1 Histogram\n",
    "\n",
    "In this exercise, we are interested in the income per capita. So let's check out the distribution of that target feature by plotting a histogram with (geo)pandas inbuilt histogram plotting functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplified quick histogram plot of target feature\n",
    "original_gdf.plot.hist(column=['hr_mtu'])\n",
    "\n",
    "#or distribution plot\n",
    "#plt.figure(figsize=(15,10))\n",
    "#sns.distplot(original_gdf['hr_mtu'],color=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Map\n",
    "As we are working with spatial data, we can also plot a map of the target feature to explore its spatial distribution.\n",
    "\n",
    "If plotting maps with matplotlib is not familiar. Here are some things you can play with\n",
    "* **figsize** - different height, width\n",
    "* **column** - try other features\n",
    "* **cmap** - this is the color map, here are the possibile options https://matplotlib.org/3.3.1/tutorials/colors/colormaps.html\n",
    "\n",
    "The following plots are only for quick visualization, to include these plots in publications, more features would need to be taken care of ( such as axes and their labels, north arrow, colorblind and print friendly color palette,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reference frame\n",
    "print(original_gdf.crs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# set title for the full plot\n",
    "ax.set_title(\"Average income by zip code\", fontsize=25)\n",
    "# turn off all axes\n",
    "ax.set_axis_off()\n",
    "# plot the average income\n",
    "plot = original_gdf.plot(column='hr_mtu', ax=ax, legend=True, cmap=\"magma\")\n",
    "# set colorbar label\n",
    "cax = fig.get_axes()[1]\n",
    "cax.set_ylabel('Income in €');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Regression plots\n",
    "\n",
    "We can also explore, how the different features are related to another by plotting them \"against each other\" by plotting some regression plots, i.e. scatter plots with a \"best fitting\" regression line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['euref_x', 'euref_y', 'he_vakiy','he_3_6']\n",
    "\n",
    "for var in variables:\n",
    "    plt.figure() # Creating a rectangle (figure) for each plot\n",
    "    # Regression Plot also by default includes best-fitting regression line which can be turned off via `fit_reg=False`\n",
    "    sns.regplot(x=var, y='hr_mtu', data=original_gdf).set(title=f'Regression plot of {var} and average income');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n",
    "\n",
    "We have a lot of features available. \n",
    "It usually make sens to check for empty rows and columns as well as empty single cells and either remove them from the dataset or, if domain knowledge allows, fill them with sensible values. Note that this might have significant impact on the results. So fill with care, and if unsure, rather remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Drop all rows that have missing values or where average income is -1 (=not known) or 0\n",
    "original_gdf = original_gdf.dropna()    \n",
    "original_gdf = original_gdf[original_gdf[\"hr_mtu\"]>0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataframe size after dropping no data rows: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n",
    "\n",
    "### Remove some columns that are strings (namn, kunta = name of the municipality in Finnish and Swedish.)\n",
    "### or which might make the modeling too easy ('hr_mtu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy')\n",
    "columns_to_be_removed_completely = ['namn','kunta','hr_ktu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy']\n",
    "original_gdf = original_gdf.drop(columns_to_be_removed_completely,axis=1)\n",
    "\n",
    "print(f\"Dataframe size after dropping columns with string values and columns that make modeling too easy : {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "The location of the zip code area does not directly influence the average per capita income. However, we know (=domain knowledge) that people with higher income can afford to live in cities and near the coast or some other water body. So let's engineer some features that represent these factors:\n",
    "\n",
    "* distance to closest city center\n",
    "* number of lakes in the area\n",
    "* distance to closest national park\n",
    "\n",
    "Be creative! Check out what other datasets you have available that could help you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# distance to closest city center (?)\n",
    "# number of lakes in the area (SYKE water areas)\n",
    "# distance to closest national park (SYKE luonnonsuejelualue)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature encoding\n",
    "\n",
    "* Most Machine Learning algorithms cannot handle categorical features per se, they have to be converted to numerical values\n",
    "* Categorical features can be binary (True/False, 1/0), ordinal (low,medium,high) or nominal (monkey, donkey, tiger, penguin)\n",
    "\n",
    "To practice, we can add region names to the post codes. One of the most-used encoding techniques is **one-hot encoding**. This means that instead of one column with different names, we create <number of unique values in column> new columns and fill then with 1/0. \n",
    "-> Same information content but numerical cells and no hierarchy (as we would get when simply assigning a numerical value to each string) \n",
    "-> also called \"dummy variables\"\n",
    "\n",
    "We use the pandas **get_dummies()** function for one-hot encoding. Scikit would also have a **OneHotEncoder()** transformer for this\n",
    "\n",
    "* More information on one-hot encoding https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "* It might not always be the best option. See other options https://towardsdatascience.com/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Spatially join the region information to the dataset \n",
    "\n",
    "First we need to bring the two dataframes together. We want to know which region each zip code are is in, so we want to \"spatially join\" the two dataframes. As the zip code areas might overlap several regions, let's choose that region for each zip code, where the mid point of each zip code polygon falls in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the regions shapefile and choose only the name of the region and its geometry\n",
    "finnish_regions_gdf = gpd.read_file(finnish_regions_shapefile)\n",
    "finnish_regions_gdf = finnish_regions_gdf[['NAMEFIN','geometry']]\n",
    "\n",
    "### A function we use to return centroid point geometry from a zip code polygon\n",
    "def returnPointGeometryFromXY(polygon_geometry):\n",
    "    ## Calculate x and y of the centroid\n",
    "    centroid_x,centroid_y = polygon_geometry.centroid.x,polygon_geometry.centroid.y\n",
    "    ## Create a shapely Point geometry of the x and y coords\n",
    "    point_geometry = Point(centroid_x,centroid_y)\n",
    "    return point_geometry\n",
    "\n",
    "### Stash the polygon geometry to another column as we are going to overwrite the 'geometry' with centroid geometry\n",
    "original_gdf['polygon_geometry'] = original_gdf['geometry']\n",
    "\n",
    "### We will be joining the region name to zip codes according to the zip code centroid. \n",
    "### This calls the function above and returns centroid to every row\n",
    "original_gdf[\"geometry\"] = original_gdf['geometry'].apply(returnPointGeometryFromXY)\n",
    "\n",
    "### Spatially join the region name to the zip codes using the centroid of zip codes and region polygons\n",
    "original_gdf = gpd.sjoin(original_gdf,finnish_regions_gdf,how='inner',op='intersects')\n",
    "# look at the end of the dataframe to see if it worked (the beginning of the dataframe has too many zip codes in same area)\n",
    "original_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 One-hot encode the region name\n",
    "\n",
    "Let's practice now the one-hot encoding on the spatially joined dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the polygon geometry back to the 'geometry' field and drop uselesss columns\n",
    "original_gdf['geometry'] = original_gdf['polygon_geometry']\n",
    "original_gdf.drop(['index_right','polygon_geometry'],axis=1, inplace=True)\n",
    "\n",
    "### Encode the region name with One-hot encoder (= in pandas, dummy encoding)\n",
    "ohencoder = OneHotEncoder()\n",
    "encoded_gdf = ohencoder.fit_transform(original_gdf['NAMEFIN'])\n",
    "\n",
    "# the pandas way for reference\n",
    "#encoded_gdf = pd.get_dummies(original_gdf['NAMEFIN'])\n",
    "\n",
    "### Join original gdf and encoded gdf together\n",
    "new_encoded_gdf = original_gdf.join(encoded_gdf).drop('NAMEFIN',axis=1)\n",
    "\n",
    "### The resulting dataframe has Polygon and Multipolygon geometries. \n",
    "### This upcasts the polygons to multipolygon format so all of them have the same format\n",
    "new_encoded_gdf[\"geometry\"] = [MultiPolygon([feature]) if type(feature) == Polygon else feature for feature in new_encoded_gdf[\"geometry\"]]\n",
    "print(\"Dataframe size after adding region name: \" + str(len(new_encoded_gdf.index))+ \" zip codes with \" + str(len(new_encoded_gdf.columns)) + \" columns\")\n",
    "\n",
    "### Print the tail of the dataframe\n",
    "new_encoded_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train- Test split\n",
    "\n",
    "In order to determine later, how well our models perform on previously unseen data we need to split the dataset into so-called train and test dataset. We use the training dataset during model training, so our regressor gets to know that dataset really well. The test dataset we only use for inference, i.e. to see how well our model can deal with previously unseen data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the gdf to x (the predictor attributes) and y (the attribute to be predicted)\n",
    "y = original_gdf['hr_mtu'] # Average income\n",
    "\n",
    "### Remove geometry and textual fields\n",
    "x = original_gdf.drop(['geometry','postinumer','nimi','hr_mtu'],axis=1)\n",
    "\n",
    "### Split both datasets to train (60%) and test (40%) datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.4, random_state=random_seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "Feature Scaling is one of the most important data preparation steps. This is to avoid biasing algorithms that compute distances between features (e.g. like KNN, SVM and other non-treebased) towards numerically larger values. Feature scaling also helps the algorithm to train and converge faster.\n",
    "The most popoular scaling techniques are normalization and standardization. Both scale the values of the current cell based on all given other cells, this means that scaling has to be done before train/test split to avoid bias towards unseen data. Apply to test set afterwards.\n",
    "\n",
    "## 7.1 Normalization or min-max scaling \n",
    "\n",
    "* X_new = (X - X_min)/(X_max - X_min)\n",
    "* Used when features are of different scales, eg average size of household (te_takk) and number of inhabitants of a certain age class (he_x_y) \n",
    "* Scales the values into range [0,1] or [-1,1]\n",
    "* Data should not have any large outliers (data exploration!), as the rest of the data will be squashed into narrow range. -> Standardization is better option\n",
    "* Scikit-learn: [MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "## 7.2 Standardization or Z-score normalization\n",
    "\n",
    "* X_new = (X - mean)/std\n",
    "* Used when \"zero mean and unit standard deviation\" needs to be ensured, we are standardizing to achieve equal variance of features\n",
    "* Not bound to specific range\n",
    "* less affected by outliers, as range is not set outliers will not have influence on the range of other values\n",
    "* \"1 implies that the value for that case is one standard deviation above the mean\"\n",
    "* Scikit-learn: [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of all column headings\n",
    "all_columns = list(original_gdf.columns)\n",
    "\n",
    "### List the column names that we don't want to be scaled, like zip-code and nimi (both regarded as string, cannot be scaled),  and geometry (cannot be scaled as is)\n",
    "col_names_no_scaling = ['postinumer','nimi','geometry']\n",
    "\n",
    "### List of column names we want to scale. (all columns minus those we don't want)\n",
    "col_names_to_scale = [column for column in all_columns if column not in col_names_no_scaling]\n",
    "\n",
    "### Subset the data for only those to-be scaled\n",
    "x_train_to_scale = x_train[col_names_to_scale]\n",
    "# we do not need to scale the label, but we also need to scale the test data\n",
    "x_test_to_scale = x_test[col_names_to_scale]\n",
    "\n",
    "\n",
    "### Apply a Scikit StandardScaler() or MinMaxScaler() for all the columns left in dataframe\n",
    "### You can also test both, store in different variables then\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_x_train_array = scaler.fit_transform(x_train_to_scale)\n",
    "\n",
    "#we also need to scale x_test with the same scaler, note that we only transform , not fit the test data\n",
    "scaled_x_test_array = scaler.transform(x_test_to_scale)\n",
    "\n",
    "\n",
    "### You can save the scaler for later use. If there suddenly would be more zip codes in Finland, we should use the same scaler.\n",
    "# dump(scaler, scaler_path, compress=True)\n",
    "\n",
    "### Result is a numpy ndarray, which we pack back into geopandas dataframe\n",
    "scaled_x_train = pd.DataFrame(scaled_x_train_array)\n",
    "scaled_x_train.columns = col_names_to_scale\n",
    "\n",
    "scaled_x_test = pd.DataFrame(scaled_x_test_array)\n",
    "scaled_x_test.columns = col_names_to_scale\n",
    "\n",
    "\n",
    "\n",
    "### Join the non-scaled columns back with the the scaled columns by index\n",
    "full_scaled_x_train = x_train[col_names_no_scaling].join(scaled_x_train)\n",
    "full_scaled_x_test = x_test[col_names_no_scaling].join(scaled_x_test)\n",
    "\n",
    "full_scaled_x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Store the results\n",
    "\n",
    "We will need the results of this notebook in two further notebooks, so we will store the prepared train and test datasets into gpkg. \n",
    "We also store the labels for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the prepared train and test zipcode datasets to geopackage\n",
    "full_scaled_x_train.to_file(train_dataset_name, driver=\"GPKG\")\n",
    "full_scaled_x_test.to_file(test_dataset_name, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "# Write the labels to pickle, as we do not need to read it outside of these notebooks, otherwise json or csv would be more compatible options\n",
    "y_train.to_pickle(train_label_name)\n",
    "y_test.to_pickle(test_label_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
