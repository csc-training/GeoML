{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector data preparations\n",
    "\n",
    "This script prepares the **Paavo zip code dataset** from the Statistics of Finland\n",
    "for machine learning purposes. \n",
    "\n",
    "It reads the original shapefile, scales all the numerical values, joins some auxiliary\n",
    "data and encodes one text field for machine learning purposes. The result is saved as geopackage.\n",
    "\n",
    "The variable descriptions of this dataset can be found here in Finnish and English\n",
    "* https://www.stat.fi/static/media/uploads/tup/paavo/paavo_lyhyt_kuvaus_2020_fi.pdf\n",
    "* https://www.stat.fi/static/media/uploads/tup/paavo/paavo_lyhyt_kuvaus_2020_en.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from joblib import dump, load\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create directories if they do not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../data']\n",
    "\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download the Paavo data from Allas with urllib and unzip it to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve('https://a3s.fi/gis-courses/gis_ml/paavo.zip', '../data/paavo.zip')\n",
    "\n",
    "with zipfile.ZipFile('../data/paavo.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_shapefile = '../data/paavo/pno_tilasto_2020.shp'\n",
    "finnish_regions_shapefile = '../data/paavo/SuomenMaakuntajako_2020_10k.shp'\n",
    "output_file_path = '../data/paavo/zip_code_data_after_preparation.gpkg'\n",
    "scaler_path = '../data/paavo/zip_code_scaler.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading and cleaning the data\n",
    "\n",
    "Read the zip code dataset into a geopandas dataframe **original_gdf** and drop unnecessary rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data from a shapefile to a geopandas dataframe\n",
    "original_gdf = gpd.read_file(zip_code_shapefile, encoding='utf-8')\n",
    "print(f\"Original dataframe size: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n",
    "\n",
    "### Drop all rows that have missing values or where average income is -1 (=not known) or 0\n",
    "original_gdf = original_gdf.dropna()    \n",
    "original_gdf = original_gdf[original_gdf[\"hr_mtu\"]>0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataframe size after dropping some rows: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n",
    "\n",
    "### Remove some columns that are strings (namn, kunta = name of the municipality in Finnish and Swedish.)\n",
    "### or which might make the modeling too easy ('hr_mtu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy')\n",
    "columns_to_be_removed_completely = ['namn','kunta','hr_ktu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy']\n",
    "original_gdf = original_gdf.drop(columns_to_be_removed_completely,axis=1)\n",
    "\n",
    "print(f\"Dataframe size after dropping some columns: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plot the geodataframe\n",
    "If plotting maps with matplotlib is not familiar. Here are some things you can play with\n",
    "* **figsize** - different height, width\n",
    "* **column** - try other zip code values\n",
    "* **cmap** - this is the color map, here are the possibile options https://matplotlib.org/3.3.1/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.set_title(\"Average income by zip code\", fontsize=25)\n",
    "ax.set_axis_off()\n",
    "original_gdf.plot(column='hr_mtu', ax=ax, legend=True, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scale the numerical columns\n",
    "Most machine learning algorithms benefit from feature scaling which means normalizing or standardizing the dataset's variablity to values between e.g. **[0-1]** or **[-1 - 1]**\n",
    "\n",
    "We do this for all numerical columns. Text (string) types of columns need different kind of treatment\n",
    "\n",
    "When to normalize or standardize?\n",
    "### Normalizing\n",
    "* Values are simply rescaled so they end up ranging from **0** to **1**. If there are huge outliers in the data, most of the variation in the data might be squashed in a narrow range of values. Standardizing might be a better idea then\n",
    "* In Scikit the transformer is called **MinMaxScaler()**\n",
    "* Some machine learning methods prefer values from **0** to **-1**\n",
    "\n",
    "### Standardizing\n",
    "* The mean value is subtracted and the value is divided with the standard deviation producing a range from **-1** to **1** where the mean is **0**. \n",
    "* In Scikit the transformer is called **StandardScaler()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of all column headings\n",
    "all_columns = list(original_gdf.columns)\n",
    "\n",
    "### List the column names that we don't want to be scaled\n",
    "col_names_no_scaling = ['postinumer','nimi','hr_mtu','geometry']\n",
    "\n",
    "### List of column names we want to scale. (all columns minus those we don't want)\n",
    "col_names_to_scaling = [column for column in all_columns if column not in col_names_no_scaling]\n",
    "\n",
    "### Subset the data for only those to-be scaled\n",
    "gdf = original_gdf[col_names_to_scaling]\n",
    "\n",
    "### Apply a Scikit StandardScaler() or MinMaxScaler() for all the columns left in dataframe\n",
    "### You can also test both \n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values_array = scaler.fit_transform(gdf)\n",
    "\n",
    "### You can save the scaler for later use with this. If there suddenly would be more zip codes in Finland, we could use the same scaler.\n",
    "dump(scaler, scaler_path, compress=True)\n",
    "\n",
    "### The scaled columns come back as a numpy ndarray, switch back to a geopandas dataframe again\n",
    "gdf = pd.DataFrame(scaled_values_array)\n",
    "gdf.columns = col_names_to_scaling\n",
    "\n",
    "### Join the non-scaled columns back with the the scaled columns by index\n",
    "scaled_gdf = original_gdf[col_names_no_scaling].join(gdf)\n",
    "scaled_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encode categorical (text) columns \n",
    "\n",
    "As an example for categorical values we add region names to post codes. The region for each post code area is retrieved from a spatial join with a regions dataset (SuomenMaankuntajako_2020_10k.shp).\n",
    "\n",
    "Machine learning algorithms do not understand text, and need different kind of pre-processing. In this excercise we use the most popular method of **one-hot encoding** (aka dummy variables) for categorical data. \n",
    "\n",
    "We use the pandas **get_dummies()** function for one-hot encoding. Scikit would also have a **OneHotEncoder()** transformer for this\n",
    "\n",
    "* More information on one-hot encoding https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "* It might not always be the best option. See other options https://towardsdatascience.com/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Spatially join the region information to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the regions shapefile and choose only the name of the region and its geometry\n",
    "finnish_regions_gdf = gpd.read_file(finnish_regions_shapefile)\n",
    "finnish_regions_gdf = finnish_regions_gdf[['NAMEFIN','geometry']]\n",
    "\n",
    "### A function we use to return centroid point geometry from a zip code polygon\n",
    "def returnPointGeometryFromXY(polygon_geometry):\n",
    "    ## Calculate x and y of the centroid\n",
    "    centroid_x,centroid_y = polygon_geometry.centroid.x,polygon_geometry.centroid.y\n",
    "    ## Create a shapely Point geometry of the x and y coords\n",
    "    point_geometry = Point(centroid_x,centroid_y)\n",
    "    return point_geometry\n",
    "\n",
    "### Stash the polygon geometry to another column as we are going to overwrite the 'geometry' with centroid geometry\n",
    "scaled_gdf['polygon_geometry'] = scaled_gdf['geometry']\n",
    "\n",
    "### We will be joining the region name to zip codes according to the zip code centroid. \n",
    "### This calls the function above and returns centroid to every row\n",
    "scaled_gdf[\"geometry\"] = scaled_gdf['geometry'].apply(returnPointGeometryFromXY)\n",
    "\n",
    "### Spatially join the region name to the zip codes using the centroid of zip codes and region polygons\n",
    "scaled_gdf = gpd.sjoin(scaled_gdf,finnish_regions_gdf,how='inner',op='intersects')\n",
    "scaled_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 One-hot encode the region name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the polygon geometry back to the 'geometry' field and drop uselesss columns\n",
    "scaled_gdf['geometry'] = scaled_gdf['polygon_geometry']\n",
    "scaled_gdf.drop(['index_right','polygon_geometry'],axis=1, inplace=True)\n",
    "\n",
    "### Encode the region name with the One-hot encoding (= in pandas, dummy encoding)\n",
    "encoded_gdf = pd.get_dummies(scaled_gdf['NAMEFIN'])\n",
    "\n",
    "### Join scaled gdf and encoded gdf together\n",
    "scaled_and_encoded_gdf = scaled_gdf.join(encoded_gdf).drop('NAMEFIN',axis=1)\n",
    "\n",
    "### The resulting dataframe has Polygon and Multipolygon geometries. \n",
    "### This upcasts the polygons to multipolygon format so all of them have the same format\n",
    "scaled_and_encoded_gdf[\"geometry\"] = [MultiPolygon([feature]) if type(feature) == Polygon else feature for feature in scaled_and_encoded_gdf[\"geometry\"]]\n",
    "print(\"Dataframe size after adding region name: \" + str(len(scaled_and_encoded_gdf.index))+ \" zip codes with \" + str(len(scaled_and_encoded_gdf.columns)) + \" columns\")\n",
    "\n",
    "### Print the tail of the dataframe\n",
    "scaled_and_encoded_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write the pre-processed zip code data to file as a Geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the prepared zipcode dataset to a geopackage\n",
    "scaled_and_encoded_gdf.to_file(output_file_path, driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
